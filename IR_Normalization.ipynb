{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Cases being handled by this Normalisation Code are -\n",
        "\n",
        "1. Normalize Punctuation Variations (e.g., U.S.A. ‚Üí USA)\n",
        "\n",
        "2. Normalize Hyphenated Words (e.g., anti-discriminatory ‚Üí antidiscriminatory)  \n",
        "\n",
        "3. Normalize Accents & Special Characters (e.g., r√©sum√© ‚Üí resume)\n",
        "\n",
        "4. Normalize Different Date Formats (e.g., 7Êúà30Êó• ‚Üí 7/30)\n",
        "\n",
        "5. Removing Stopwords\n",
        "\n",
        "6. Lemmatization\n",
        "\n",
        "7. Normalize Proper Nouns & Case Variations (e.g., General Motors ‚Üí general\n",
        "motors)\n",
        "8. Normalize Language Variations (e.g., T√ºbingen ‚Üí Tubingen)\n",
        "\n",
        "9. Normalize Extra Whitespace (e.g., Hello World ‚Üí Hello World)\n",
        "\n",
        "10. Normalize Contractions (e.g., can't ‚Üí cannot, it's ‚Üí it is)\n"
      ],
      "metadata": {
        "id": "TEE7sP1ekh-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-71VSQjAixA4",
        "outputId": "0e2bd600-87ba-4925-8872-07fc1c041249"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_3JHnldi53o",
        "outputId": "46969fc7-7b01-4045-9ddc-20e7ec81986d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMrkNuolhCxh",
        "outputId": "0e6ba9f1-175c-4818-d4d8-93b78d7e263e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "from dateutil import parser\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import inflect\n",
        "from contractions import fix\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PYxZE2-gzJh",
        "outputId": "16fb887d-faf5-433d-c060-d52ed431e636"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Normalize Punctuation Variations**"
      ],
      "metadata": {
        "id": "US5O8xA7kh3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def normalize_punctuation(text):\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation.replace(\"-\", \"\").replace(\".\", \"\")))\n",
        "    # Remove dots within words but preserve decimal points and email addresses\n",
        "    text = re.sub(r'\\b(\\w(?:\\.\\w)+)\\b', lambda x: x.group(1).replace('.', ''), text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example Test Cases\n",
        "input_text = input(\"Enter a text: \")\n",
        "print(\"Normalized Text:\", normalize_punctuation(input_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKzBZkIxg0f7",
        "outputId": "5de701fa-e252-4448-9da4-177bb95123ba"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text: a\n",
            "Normalized Text: a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Normalize Accents & Special Characters**"
      ],
      "metadata": {
        "id": "_Ic8nKiVkhrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_accents(text):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFKD', text) if not unicodedata.combining(c))\n",
        "\n",
        "input_text = input(\"Enter a text: \")\n",
        "print(\"Normalized Text:\", normalize_accents(input_text))\n",
        "\n",
        "## e.g., r√©sum√© ‚Üí resume"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0plsY4VhgS7",
        "outputId": "66ff6e28-ea6d-4468-f5e0-e085d609a9af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text: a\n",
            "Normalized Text: a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Removing Stopwords**"
      ],
      "metadata": {
        "id": "XqD9LoLAYJYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def normalize_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    return ' '.join([word for word in words if word.lower() not in stop_words])\n",
        "\n",
        "input_text = input(\"Enter text: \")\n",
        "print(\"Stopword-Normalized Text:\", normalize_stopwords(input_text))\n",
        "#This is a good day to go outside."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGO85cZlVs_1",
        "outputId": "abcf16d2-2f99-495a-b941-62c94bc5df40"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text: a\n",
            "Stopword-Normalized Text: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Lemmatization**"
      ],
      "metadata": {
        "id": "jC9hhan5YP6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def normalize_lemmatization(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = word_tokenize(text)\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "input_text = input(\"Enter text: \")\n",
        "print(\"Lemmatized Text:\", normalize_lemmatization(input_text))\n",
        "#running dogs are happier than sleeping ones"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga-NaFuQWW51",
        "outputId": "f120cbff-70b2-4817-9014-b79727dfd64c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text: a\n",
            "Lemmatized Text: a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dateparser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Isvcp0_qnoQC",
        "outputId": "63ec7d41-99f8-4cdd-89e1-8a63089b19e2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dateparser\n",
            "  Downloading dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2025.1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser) (5.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->dateparser) (1.17.0)\n",
            "Downloading dateparser-1.2.1-py3-none-any.whl (295 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/295.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dateparser\n",
            "Successfully installed dateparser-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Normalize Different Date Formats**"
      ],
      "metadata": {
        "id": "HprvChL0khlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dateparser\n",
        "\n",
        "def normalize_dates(text):\n",
        "    # Find date-like patterns and replace them\n",
        "    words = text.split()\n",
        "    for i, word in enumerate(words):\n",
        "        try:\n",
        "            date_obj = dateparser.parse(word, languages=['en', 'ja'])\n",
        "            if date_obj:\n",
        "                words[i] = date_obj.strftime(\"%m/%d/%Y\")\n",
        "        except:\n",
        "            continue\n",
        "    return \" \".join(words)"
      ],
      "metadata": {
        "id": "MVCkDu--hlB6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Normalize Proper Nouns & Case Variations**"
      ],
      "metadata": {
        "id": "ajUKmKUfkhe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_case(text):\n",
        "    return text.lower()\n",
        "\n",
        "input_text = input(\"Enter a text: \")\n",
        "print(\"Normalized Text:\", normalize_case(input_text))\n",
        "\n",
        "\n",
        "## e.g., General Motors ‚Üí general motors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PezGLh92hpb-",
        "outputId": "c868496f-182c-483a-abf2-d924956adb70"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text: a\n",
            "Normalized Text: a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Normalize Language Variations**"
      ],
      "metadata": {
        "id": "I12qKkSgkhYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_language_variations(text):\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
        "\n",
        "input_text = input(\"Enter a text: \")\n",
        "print(\"Normalized Text:\", normalize_language_variations(input_text))\n",
        "\n",
        "\n",
        "## e.g., T√ºbingen ‚Üí Tubingen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7WYdpp3h_r7",
        "outputId": "ef33fd6e-cc59-4301-d7dc-649a848ddb46"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text: a\n",
            "Normalized Text: a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Normalize Extra Whitespace**"
      ],
      "metadata": {
        "id": "X3hpSpt-khSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_whitespace(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "input_text = input(\"Enter a text: \")\n",
        "print(\"Normalized Text:\", normalize_whitespace(input_text))\n",
        "\n",
        "\n",
        "## (e.g., Hello     World ‚Üí Hello World"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MarcbQZViELO",
        "outputId": "f1c0a80d-e6fc-4a0f-bf35-ccb5b416617c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text: a     a\n",
            "Normalized Text: a a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Normalize Contractions**"
      ],
      "metadata": {
        "id": "RBmwT9IBkhL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_contractions(text):\n",
        "    return fix(text)\n",
        "\n",
        "input_text = input(\"Enter a text: \")\n",
        "print(\"Normalized Text:\", normalize_contractions(input_text))\n",
        "\n",
        "## e.g., can't ‚Üí cannot, it's ‚Üí it is"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoC41pF8iHo2",
        "outputId": "cbc75a54-f66a-48bc-a9b2-c0a3f04b9582"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text: can't\n",
            "Normalized Text: cannot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalize Full Sentence**"
      ],
      "metadata": {
        "id": "DusW-enrmsZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_all(text):\n",
        "    text = normalize_punctuation(text)\n",
        "    text = normalize_stopwords(text)\n",
        "    text = normalize_lemmatization(text)\n",
        "    text = normalize_accents(text)\n",
        "    text = normalize_dates(text)\n",
        "    text = normalize_case(text)\n",
        "    text = normalize_language_variations(text)\n",
        "    text = normalize_contractions(text)\n",
        "    text = normalize_whitespace(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "input_text = input(\"Enter a text: \")\n",
        "print(\"\\n\\n\")  # Adds a clean double line break for spacing\n",
        "print(\"Fully Normalized Text:\", normalize_all(input_text))\n",
        "\n",
        "\n",
        "\n",
        "## Sample text\n",
        "\n",
        "## \"U.S.A and USA are the same. The anti-discriminatory policies were updated. His r√©sum√© was impressive, just like G√©n√©ral Motors'. T√ºbingen and Tubingen are the same place. He was born on 7Êúà30Êó•2025, while another record states 30-07-2023. SAIL and sail refer to different things, but co-operate and cooperate mean the same.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_elAC6N9iQ_r",
        "outputId": "b0bbb6a7-bd3c-4d4a-d42b-a36d8c9f85d1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text: U.S.A and USA are the same. The anti-discriminatory policies were updated. His r√©sum√© was impressive, just like G√©n√©ral Motors'. T√ºbingen and Tubingen are the same place. He was born on 7Êúà30Êó•2025, while another record states 30-07-2023. SAIL and sail refer to different things, but co-operate and cooperate mean the same.\"\n",
            "\n",
            "\n",
            "\n",
            "Fully Normalized Text: usa usa . anti-discriminatory policy updated . resume impressive , like general motors ' . tubingen tubingen place . born 7302025 , another record state 07/30/2023 . sail sail refer different thing , co-operate cooperate mean . ``\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQRe1TZfNggd",
        "outputId": "bd411fa7-5f1d-41f1-c7f5-53c45805ab2e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalizing using Soundex**"
      ],
      "metadata": {
        "id": "1nddCs-IYgmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Soundex Implementation\n",
        "def soundex(word):\n",
        "    if not word:\n",
        "        return \"\"\n",
        "    word = word.upper()\n",
        "    soundex_mapping = {\n",
        "        \"B\": \"1\", \"F\": \"1\", \"P\": \"1\", \"V\": \"1\",\n",
        "        \"C\": \"2\", \"G\": \"2\", \"J\": \"2\", \"K\": \"2\", \"Q\": \"2\", \"S\": \"2\", \"X\": \"2\", \"Z\": \"2\",\n",
        "        \"D\": \"3\", \"T\": \"3\",\n",
        "        \"L\": \"4\",\n",
        "        \"M\": \"5\", \"N\": \"5\",\n",
        "        \"R\": \"6\"\n",
        "    }\n",
        "\n",
        "    first_letter = word[0]  # Keep the first letter\n",
        "    encoded = first_letter\n",
        "\n",
        "    # Convert remaining letters\n",
        "    for char in word[1:]:\n",
        "        if char in soundex_mapping:\n",
        "            digit = soundex_mapping[char]\n",
        "            if digit != encoded[-1]:  # Avoid consecutive duplicates\n",
        "                encoded += digit\n",
        "\n",
        "    encoded = encoded.replace(\"0\", \"\")  # Remove zeros\n",
        "    encoded = (encoded + \"000\")[:4]  # Ensure length is 4\n",
        "    return encoded\n",
        "\n"
      ],
      "metadata": {
        "id": "B1EHp8bpq7AP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test sentence\n",
        "sentence = \"The r√©sum√© of Dr. J.K.L. is well-known. He visited U.S.A. on 12Êúà25Êó•, and researched anti-discriminatory policies for Windows-based systems.\"\n",
        "\n",
        "# Apply Soundex to Each Word\n",
        "soundex_codes = {word: soundex(word) for word in sentence.split()}\n",
        "\n",
        "# Print results\n",
        "print(\"\\nüîπ Original Sentence:\")\n",
        "print(sentence)\n",
        "\n",
        "print(\"\\nüîπ Soundex Codes:\")\n",
        "for word, code in soundex_codes.items():\n",
        "    print(f\"{word}:¬†{code}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGrGnsBgMc--",
        "outputId": "e52645e5-0244-4765-93c2-e5d7ee9bbf49"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ Original Sentence:\n",
            "The r√©sum√© of Dr. J.K.L. is well-known. He visited U.S.A. on 12Êúà25Êó•, and researched anti-discriminatory policies for Windows-based systems.\n",
            "\n",
            "üîπ Soundex Codes:\n",
            "The:¬†T000\n",
            "r√©sum√©:¬†R250\n",
            "of:¬†O100\n",
            "Dr.:¬†D600\n",
            "J.K.L.:¬†J240\n",
            "is:¬†I200\n",
            "well-known.:¬†W425\n",
            "He:¬†H000\n",
            "visited:¬†V230\n",
            "U.S.A.:¬†U200\n",
            "on:¬†O500\n",
            "12Êúà25Êó•,:¬†1000\n",
            "and:¬†A530\n",
            "researched:¬†R262\n",
            "anti-discriminatory:¬†A532\n",
            "policies:¬†P420\n",
            "for:¬†F600\n",
            "Windows-based:¬†W532\n",
            "systems.:¬†S235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_with_soundex(text):\n",
        "    text = normalize_all(text)  # Apply all normalizations\n",
        "    codes = [soundex(word) for word in sentence.split()]  # Convert each word to Soundex\n",
        "    return ' '.join(codes)\n",
        "\n",
        "# **Example Run**\n",
        "input_text = input(\"Enter a text: \")\n",
        "print(\"\\n\\n\")\n",
        "print(\"Fully Normalized Text:\", normalize_all(input_text))\n",
        "print(\"Soundex Normalized Text:\", normalize_with_soundex(input_text))\n",
        "\n",
        "## Sample text\n",
        "\n",
        "## \"U.S.A and USA are the same. The anti-discriminatory policies were updated. His r√©sum√© was impressive, just like G√©n√©ral Motors'. T√ºbingen and Tubingen are the same place. He was born on 7Êúà30Êó•, while another record states 30-07-2023. SAIL and sail refer to different things, but co-operate and cooperate mean the same.\"\n"
      ],
      "metadata": {
        "id": "4PjVogkiZi2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d3bd197-fcf5-4c2b-c6cd-695117485639"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text: a\n",
            "\n",
            "\n",
            "\n",
            "Fully Normalized Text: \n",
            "Soundex Normalized Text: T000 R250 O100 D600 J240 I200 W425 H000 V230 U200 O500 1000 A530 R262 A532 P420 F600 W532 S235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalisation using metaphone"
      ],
      "metadata": {
        "id": "3X9Y7NWNzoo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install metaphone"
      ],
      "metadata": {
        "id": "lCF48edBPSL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc201914-0e36-4df5-fab4-7baecc31acce"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting metaphone\n",
            "  Downloading Metaphone-0.6.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: metaphone\n",
            "  Building wheel for metaphone (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for metaphone: filename=Metaphone-0.6-py3-none-any.whl size=13901 sha256=a229a34839347f2d648f46d5a8b9a2a0e1cee4c7a420de9daf1814b5e9ffeed3\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/cb/f9/3ce2de290cd1b6f10dd8ed4795f3dec4a835b02d2514f9b9d3\n",
            "Successfully built metaphone\n",
            "Installing collected packages: metaphone\n",
            "Successfully installed metaphone-0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from metaphone import doublemetaphone\n",
        "\n",
        "def normalize_with_metaphone(text):\n",
        "    \"\"\"\n",
        "    Normalizes text by converting each word into its Metaphone encoding.\n",
        "\n",
        "    Args:\n",
        "    - text (str): Input sentence.\n",
        "\n",
        "    Returns:\n",
        "    - str: Sentence with words replaced by their Metaphone encoding.\n",
        "    \"\"\"\n",
        "    text = normalize_all(text)  # Apply all normalizations (assumed to be defined)\n",
        "    words = text.split()\n",
        "    codes = [doublemetaphone(word)[0] for word in words]  # Convert words to Metaphone primary encoding\n",
        "    return ' '.join(codes)\n",
        "\n",
        "# **Example Run**\n",
        "input_text = input(\"Enter a text: \")\n",
        "print(\"\\n\\n\")\n",
        "print(\"Fully Normalized Text:\", normalize_all(input_text))\n",
        "print(\"Metaphone Normalized Text:\", normalize_with_metaphone(input_text))\n"
      ],
      "metadata": {
        "id": "jOurOkeI1Vy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89096c22-0516-435a-9585-f762701b0aa7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text: a\n",
            "\n",
            "\n",
            "\n",
            "Fully Normalized Text: \n",
            "Metaphone Normalized Text: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalising a file"
      ],
      "metadata": {
        "id": "Vo0uHKuV38_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def process_file(input_file, norm_file):\n",
        "    \"\"\"\n",
        "    Reads a file, applies text normalization & Metaphone normalization,\n",
        "    then saves both versions to separate files.\n",
        "    \"\"\"\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    normalized_text = normalize_all(text)\n",
        "    metaphone_text = normalize_with_metaphone(text)\n",
        "\n",
        "    phonetic_file = norm_file.replace(\".txt\", \"_metaphone.txt\")  # Auto-generate Metaphone file\n",
        "\n",
        "    with open(norm_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(normalized_text)\n",
        "\n",
        "    with open(phonetic_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(metaphone_text)\n",
        "\n",
        "    return normalized_text, metaphone_text, phonetic_file\n",
        "\n",
        "\n",
        "# **Manually Enter File Path Instead of Using Tkinter**\n",
        "input_filename = input(\"Enter the full path of the input text file: \")\n",
        "\n",
        "# **Check if File Exists**\n",
        "if not os.path.exists(input_filename):\n",
        "    print(\"‚ùå Error: File not found. Please check the path and try again.\")\n",
        "else:\n",
        "    output_filename = input(\"Enter the output filename for normalized text (without extension): \") + \".txt\"\n",
        "\n",
        "    # Process the file\n",
        "    normalized_text, metaphone_text, metaphone_file = process_file(input_filename, output_filename)\n",
        "\n",
        "    # **Preview of Results**\n",
        "    print(\"\\n--- Normalized Text Preview ---\\n\")\n",
        "    print(normalized_text[:500])  # Show first 500 characters\n",
        "\n",
        "    print(\"\\n--- Metaphone Normalized Text Preview ---\\n\")\n",
        "    print(metaphone_text[:500])  # Show first 500 characters\n",
        "\n",
        "    print(f\"\\n‚úÖ Normalized text saved in: {output_filename}\")\n",
        "    print(f\"‚úÖ Metaphone normalized text saved in: {metaphone_file}\")\n"
      ],
      "metadata": {
        "id": "vdS6xhCf1c7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25a3999-ab71-49bd-b959-730750b66449"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the full path of the input text file: /content/tata_innovent.txt\n",
            "Enter the output filename for normalized text (without extension): samp\n",
            "\n",
            "--- Normalized Text Preview ---\n",
            "\n",
            "introduction introduce team provide brief overview project . problem statement clearly define problem challenge project aim address along market size future potential . objective approach state project main objective describe approach methodology used tackle problem address challenge . solution overview present detailed overview solution highlighting key component functionality . use multiple slide showcase solution novelty . challenges faced discuss challenge encountered project development ove\n",
            "\n",
            "--- Metaphone Normalized Text Preview ---\n",
            "\n",
            "ANTRTKXN ANTRTS TM PRFT PRF AFRF PRJKT  PRPLM STTMNT KLRL TFN PRPLM XLNJ PRJKT AM ATRS ALNK MRKT SS FTR PTNXL  APJKTF APRX STT PRJKT MN APJKTF TSKP APRX M0TLJ AST TKL PRPLM ATRS XLNJ  SLXN AFRF PRSNT TTLT AFRF SLXN HHLTNK K KMPNNT FNKXNLT  AS MLTPL SLT XKS SLXN NFLT  XLNJS FST TSKS XLNJ ANKNTRT PRJKT TFLPMNT AFRKM  TKNKL AMPLMNTXN AKSPLN TKNKL ASPKT PRJKT ANKLTNK TKNLJ TL FRMRK AST  RSLTS AXFMNTS XKS ATKM AXFMNT AMPKT PRJKT SPRTT TT AFTNS  TMNSTRXN PRFT RKRTT TMNSTRXN TJTL FSKL PRTTP ANFXN XKSNK\n",
            "\n",
            "‚úÖ Normalized text saved in: samp.txt\n",
            "‚úÖ Metaphone normalized text saved in: samp_metaphone.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vsRpHcfh5Uuv"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}